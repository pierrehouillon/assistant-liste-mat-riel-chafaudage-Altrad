import OpenAI from "openai";
import { StreamingTextResponse, OpenAIStream } from "ai";

// ‚ö†Ô∏è Mets ici le nom EXACT de ton vector store OpenAI
const VECTOR_STORE_ID = "altrad-metrix-knowledge";

export const runtime = "edge";

const client = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

/**
 * Extraction d‚Äô√©tat : on analyse tout l'historique de la conversation
 * pour r√©cup√©rer longueur, hauteur, largeur, protection mur, grutage.
 */
function extractState(messages) {
  const state = {
    longueur: null,
    hauteur: null,
    largeur: null,
    protectionMur: null,
    grutage: null,
  };

  for (const m of messages) {
    const txt = m.content.toLowerCase();

    // LONGUEUR
    const lg = txt.match(/(\d+[.,]?\d*)\s*m(?:√®tre)?s?\s*(?:de long|de fa√ßade|longueur)/);
    if (lg) state.longueur = parseFloat(lg[1].replace(",", "."));

    // HAUTEUR
    const ht = txt.match(/(\d+[.,]?\d*)\s*m(?:√®tre)?s?\s*(?:de haut|hauteur)/);
    if (ht) state.hauteur = parseFloat(ht[1].replace(",", "."));

    // LARGEUR explicit√©e
    if (txt.includes("0,70") || txt.includes("0.70") || txt.includes("70cm")) state.largeur = 0.7;
    if (txt.includes("1m") || txt.includes("1 m") || txt.includes("1.00")) state.largeur = 1;

    // PROTECTION C√îT√â MUR
    if (txt.includes("protection") && txt.includes("mur")) {
      if (txt.includes("oui")) state.protectionMur = true;
      if (txt.includes("non")) state.protectionMur = false;
    }

    // GRUTAGE
    if (txt.includes("grut")) {
      if (txt.includes("oui")) state.grutage = true;
      if (txt.includes("non")) state.grutage = false;
    }
  }

  return state;
}


/**
 * G√©n√®re le message syst√®me enrichi (instructions + √©tat m√©moire).
 */
function buildSystemPrompt(state) {
  return `
Tu es ALTRAD Assistant METRIX.
Tu dois utiliser les documents du vector store "${VECTOR_STORE_ID}" pour r√©pondre.
Ton r√¥le : guider le collaborateur jusqu'√† une liste compl√®te de mat√©riel METRIX Peduzzi.

√âTAT ACTUEL :
- Longueur : ${state.longueur ?? "inconnue"}
- Hauteur : ${state.hauteur ?? "inconnue"}
- Largeur : ${state.largeur ?? "inconnue"}
- Protection c√¥t√© mur : ${state.protectionMur ?? "inconnue"}
- Grutage : ${state.grutage ?? "inconnue"}

R√àGLES DE DIALOGUE :
- Ne JAMAIS reposer une question d√©j√† r√©pondue.
- Si l‚Äôutilisateur parle de m¬≤ : demander longueur + hauteur, sans proposer de valeurs.
- Toujours poser les questions restantes dans cet ordre :
  1) longueur
  2) hauteur
  3) largeur (si pas d√©j√† donn√©e ‚Äì par d√©faut 1 m)
  4) protection c√¥t√© mur (rappeler que >20 cm = obligatoire)
  5) grutage
- Quand tout est connu : produire imm√©diatement la liste de mat√©riel (format tableau HTML).

RAPPEL :
Tu t‚Äôappuies sur les documents du vector store pour toutes les r√®gles (notice fabricant + catalogue Peduzzi + instructions).
Ne jamais inventer une r√©f√©rence.
R√©ponds toujours comme un coll√®gue technique exp√©riment√©.
`;
}


/**
 * ENDPOINT API /api/chat
 */
export async function POST(req) {
  try {
    const body = await req.json();
    const userMessages = body.messages || [];

    // üîç On reconstruit l'√©tat depuis l'historique
    const state = extractState(userMessages);

    // üîß Cr√©ation du message syst√®me enrichi
    const systemMessage = {
      role: "system",
      content: buildSystemPrompt(state),
    };

    // Construction du flux complet
    const finalMessages = [systemMessage, ...userMessages];

    // üî• Appel OpenAI avec r√©cup√©ration automatique dans ton vector store
    const response = await client.chat.completions.create({
      model: "gpt-4o-mini", // ou "gpt-4.1" si tu veux encore plus solide
      messages: finalMessages,
      stream: true,
      retrieval: {
        vector_store_ids: [VECTOR_STORE_ID],
      },
    });

    // Flux streaming vers le front
    const stream = OpenAIStream(response);
    return new StreamingTextResponse(stream);

  } catch (err) {
    console.error("‚ùå ERREUR API CHAT :", err);
    return new Response("Erreur interne", { status: 500 });
  }
}
